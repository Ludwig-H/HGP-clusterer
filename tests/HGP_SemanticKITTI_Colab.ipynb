{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "HGP_SemanticKITTI_local.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HGP clusterer sur SemanticKITTI (local)\n",
        "Ce notebook prépare l'environnement, récupère **hgp_clusterer** depuis votre GitHub, lance le clustering **en local** sur le benchmark **SemanticKITTI**, enregistre les prédictions panoptiques et affiche un aperçu des résultats.\n",
        "\n",
        "- Même **pré-traitements** que dans vos notebooks (sélection par `PREPROC`: `none`, `bev_xy`, `bev_xyzi`, `polar`)\n",
        "- **Aucun post-traitement** (pas de KNN, pas de merges, pas de box split)\n",
        "- Sauvegarde des `.label` panoptiques dans la structure attendue par SemanticKITTI\n",
        "- Évaluation THINGS-only optionnelle via `semantic-kitti-api`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 0) Dépendances système et Python\n",
        "%%bash\n",
        "set -euo pipefail\n",
        "apt-get update -qq\n",
        "# CGAL/TBB/CMake et dépendances classiques\n",
        "apt-get install -y -qq build-essential cmake libcgal-dev libtbb-dev libtbbmalloc2   libgmp-dev libmpfr-dev libeigen3-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quelques modules Python utiles\n",
        "!pip -q install --upgrade pip setuptools wheel Cython cmake jedi\n",
        "!pip -q install numpy scipy scikit-learn plotly tqdm joblib\n",
        "# shapely n'est pas requis sans post-processing, on l'évite volontairement ici"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 1) Récupération du dépôt hgp_clusterer (GitHub) + cyminiball + CGALDelaunay\n",
        "import os, subprocess, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# === Paramètre clé : URL du dépôt Git de hgp_clusterer ===\n",
        "# Remplacez par l'URL de votre dépôt si besoin.\n",
        "REPO_URL = os.environ.get(\"HGP_REPO_URL\", \"https://github.com/Ludwig-H/HGP-clusterer.git\")\n",
        "\n",
        "WORKDIR = Path(os.environ.get(\"HGP_WORKDIR\", \"/content\")).resolve()\n",
        "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_dir = WORKDIR / \"HGP-clusterer\"\n",
        "cymini_dir = WORKDIR / \"cyminiball\"\n",
        "\n",
        "def _run(cmd, **kw):\n",
        "    print(\"+\", cmd)\n",
        "    subprocess.run(cmd, check=True, **kw)\n",
        "\n",
        "# Clone / update hgp_clusterer\n",
        "if repo_dir.exists():\n",
        "    _run([\"git\", \"-C\", str(repo_dir), \"pull\", \"--ff-only\"])\n",
        "else:\n",
        "    _run([\"git\", \"clone\", REPO_URL, str(repo_dir)])\n",
        "\n",
        "# cyminiball (lib légère utilisée par le projet)\n",
        "if cymini_dir.exists():\n",
        "    _run([\"git\", \"-C\", str(cymini_dir), \"pull\", \"--ff-only\"])\n",
        "else:\n",
        "    _run([\"git\", \"clone\", \"https://github.com/Ludwig-H/cyminiball.git\", str(cymini_dir)])\n",
        "\n",
        "# Build + install cyminiball via wheel local (fiable en Colab)\n",
        "wheels = WORKDIR / \"wheels\"\n",
        "wheels.mkdir(parents=True, exist_ok=True)\n",
        "_run([sys.executable, \"-m\", \"pip\", \"wheel\", \"--no-build-isolation\", \"--no-deps\", \"--wheel-dir\", str(wheels), str(cymini_dir)])\n",
        "_run([sys.executable, \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"--no-deps\", \"--no-index\", f\"--find-links={wheels}\", \"cyminiball\"])\n",
        "\n",
        "# Compile les binaires CGALDelaunay et installe dans le dépôt\n",
        "cgal_root = repo_dir / \"CGALDelaunay\"\n",
        "projects = [\n",
        "    \"EdgesCGALDelaunay2D\",\n",
        "    \"EdgesCGALDelaunay3D\",\n",
        "    \"EdgesCGALDelaUNayND\".replace(\"UN\", \"N\"),  # safe typo fix\n",
        "    \"EdgesCGALWeightedDelaunay2D\",\n",
        "    \"EdgesCGALWeightedDelaunay3D\",\n",
        "    \"EdgesCGALWeightedDelaunayND\",\n",
        "]\n",
        "for p in projects:\n",
        "    src = cgal_root / p\n",
        "    bld = src / \"build\"\n",
        "    bld.mkdir(parents=True, exist_ok=True)\n",
        "    _run([\"cmake\", \"-S\", str(src), \"-B\", str(bld), \"-DCMAKE_BUILD_TYPE=Release\"])\n",
        "    _run([\"cmake\", \"--build\", str(bld), \"--config\", \"Release\", \"-j\"])\n",
        "    _run([\"cmake\", \"--install\", str(bld), \"--prefix\", str(repo_dir)])\n",
        "\n",
        "# Variable d'environnement requise par hgp_clusterer\n",
        "os.environ[\"CGALDELAUNAY_ROOT\"] = str(cgal_root)\n",
        "print(\"CGALDELAUNAY_ROOT =\", os.environ[\"CGALDELAUNAY_ROOT\"])\n",
        "\n",
        "print(\"Dépôt hgp_clusterer prêt: \", repo_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 2) Imports + configuration centrale\n",
        "import os, json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict\n",
        "\n",
        "# Import coeur HGP\n",
        "repo_dir = Path(os.environ.get(\"HGP_WORKDIR\", \"/content\")) / \"HGP-clusterer\"\n",
        "os.environ[\"CGALDELAUNAY_ROOT\"] = str(repo_dir / \"CGALDelaunay\")\n",
        "from hgp_clusterer import HypergraphPercol\n",
        "\n",
        "# === Dossiers ===\n",
        "DATA_ROOT = \"/content/drive/MyDrive/Datasets/semantic_kitti\"  #@param {type:\"string\"}\n",
        "# Sémantique pour le clustering: \"oracle\" (vérité terrain), \"waffleiron\" (preds), \"custom\" (vos propres prédictions)\n",
        "SEMANTICS_SOURCE = \"waffleiron\"  #@param [\"oracle\", \"waffleiron\", \"custom\"]\n",
        "# Si \"waffleiron\" ou \"custom\": racine contenant sequences/*/predictions/*.{label,labels,npy,npz}\n",
        "PRED_ROOT = \"/content/drive/MyDrive/Datasets/semantic_kitti/WaffleIron\"  #@param {type:\"string\"}\n",
        "\n",
        "# === Séquences à traiter ===\n",
        "SEQUENCES = [\"08\"]  #@param {type:\"raw\"}\n",
        "\n",
        "# === Paramètres HGP (identiques à votre notebook local) ===\n",
        "K = 5  #@param {type:\"integer\"}\n",
        "min_cluster_size = 50  #@param {type:\"integer\"}\n",
        "min_samples = K + 1    #@param {type:\"integer\"}\n",
        "method = \"eom\"         #@param [\"eom\",\"leaf\"]\n",
        "splitting = None       #@param [\"None\", \"balanced\", \"tight\"]\n",
        "weight_face = \"lambda\" #@param [\"lambda\",\"uniform\",\"unique\"]\n",
        "label_all_points = False  #@param {type:\"boolean\"}\n",
        "return_multi_clusters = False  #@param {type:\"boolean\"}\n",
        "complex_chosen = \"orderk_delaunay\"  #@param [\"orderk_delaunay\",\"delaunay\",\"weighted_delaunay\"]\n",
        "expZ = 3  #@param {type:\"integer\"}\n",
        "HGP_VERBOSE = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# === Pré-traitements (mêmes options) — pas de post-traitement dans ce notebook ===\n",
        "PREPROC = \"bev_xy\"  #@param [\"none\",\"bev_xy\",\"bev_xyzi\",\"polar\"]\n",
        "\n",
        "# === Sortie: même structure que SemanticKITTI leaderboard ===\n",
        "RUN_NAME = f\"HGP-K{K}_min{min_cluster_size}_expZ{expZ}_pre{PREPROC}_NOPP\"\n",
        "OUT_ROOT = f\"/content/drive/MyDrive/Datasets/semantic_kitti/experiments_semkitti/{RUN_NAME}\"\n",
        "Path(OUT_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "print(\"Sortie:\", OUT_ROOT)\n",
        "\n",
        "# === THINGS/ STUFF (labels bruts SemanticKITTI) ===\n",
        "THING_RAW_IDS = [10,11,15,18,20,30,31,32]  # car, bicycle, motorcycle, truck, other-vehicle, person, bicyclist, motorcyclist\n",
        "STUFF_RAW_IDS = [40,44,48,49,50,51,70,71,72,80,81]\n",
        "\n",
        "# Mapping SemanticKITTI -> 20 classes entraînement (utilisé pour l'évaluation)\n",
        "MAPPER = {0: 0, 1: 0, 10: 1, 11: 2, 13: 5, 15: 3, 16: 5, 18: 4, 20: 5, 30: 6, 31: 7, 32: 8,\n",
        "          40: 9, 44: 10, 48: 11, 49: 12, 50: 13, 51: 14, 52: 0, 60: 9,\n",
        "          70: 15, 71: 16, 72: 17, 80: 18, 81: 19, 99: 0, 252: 1, 253: 7, 254: 6,\n",
        "          255: 8, 256: 5, 257: 5, 258: 4, 259: 5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 3) Utilitaires I/O (KITTI) + encodage panoptique\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "def kitti_scan_paths(seq_dir: Path):\n",
        "    vel_dir = seq_dir / \"velodyne\"\n",
        "    label_dir = seq_dir / \"labels\"\n",
        "    assert vel_dir.is_dir(), f\"Manque velodyne sous {seq_dir}\"\n",
        "    stems = sorted([p.stem for p in vel_dir.glob(\"*.bin\")])\n",
        "    return vel_dir, label_dir, stems\n",
        "\n",
        "def read_points_bin(bin_path: Path) -> np.ndarray:\n",
        "    arr = np.fromfile(str(bin_path), dtype=np.float32)\n",
        "    return arr.reshape(-1, 4)  # x,y,z,remission\n",
        "\n",
        "def read_label_file(label_path: Path) -> np.ndarray:\n",
        "    # 32-bit uint: upper 16 bits = instance id, lower 16 = semantic id\n",
        "    return np.fromfile(str(label_path), dtype=np.uint32)\n",
        "\n",
        "def pack_panoptic(semantic: np.ndarray, instance: np.ndarray) -> np.ndarray:\n",
        "    assert semantic.shape == instance.shape\n",
        "    return ((instance.astype(np.uint32) << 16) | (semantic.astype(np.uint32)))\n",
        "\n",
        "def unpack_semantic(label32: np.ndarray) -> np.ndarray:\n",
        "    return (label32 & 0xFFFF).astype(np.uint16)\n",
        "\n",
        "def _find_pred_file(pred_dir: Path, stem: str) -> Optional[Path]:\n",
        "    # Supporte .label, .labels, .npy, .npz\n",
        "    for ext in (\".label\", \".labels\", \".npy\", \".npz\"):\n",
        "        p = pred_dir / f\"{stem}{ext}\"\n",
        "        if p.is_file():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def _load_semantics_generic(pred_path: Path) -> np.ndarray:\n",
        "    if pred_path.suffix in (\".label\", \".labels\"):\n",
        "        u32 = np.fromfile(str(pred_path), dtype=np.uint32)\n",
        "        return (u32 & 0xFFFF).astype(np.uint16)\n",
        "    if pred_path.suffix == \".npy\":\n",
        "        arr = np.load(str(pred_path))\n",
        "        arr = np.array(arr).reshape(-1)\n",
        "        return arr.astype(np.uint16)\n",
        "    if pred_path.suffix == \".npz\":\n",
        "        data = np.load(str(pred_path))\n",
        "        # prend la première clé par défaut\n",
        "        key = list(data.keys())[0]\n",
        "        arr = np.array(data[key]).reshape(-1)\n",
        "        return arr.astype(np.uint16)\n",
        "    raise ValueError(f\"Extension non supportée: {pred_path}\")\n",
        "\n",
        "def load_semantics_for_scan(seq_root: Path, stem: str) -> np.ndarray:\n",
        "    if SEMANTICS_SOURCE == \"oracle\":\n",
        "        _, label_dir, _ = kitti_scan_paths(seq_root)\n",
        "        gt32 = read_label_file(label_dir / f\"{stem}.label\")\n",
        "        return unpack_semantic(gt32)\n",
        "    else:\n",
        "        pred_dir = Path(PRED_ROOT) / \"sequences\" / seq_root.name / \"predictions\"\n",
        "        pred_file = _find_pred_file(pred_dir, stem)\n",
        "        assert pred_file is not None, f\"Prediction manquante: {pred_dir}/{stem}.*\"\n",
        "        return _load_semantics_generic(pred_file)\n",
        "\n",
        "print(\"I/O utils chargés.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 4) Pré-traitements (features HGP) — aucun post-traitement ici\n",
        "import numpy as np\n",
        "\n",
        "def compute_features(points_xyzi: np.ndarray, mode: str = \"none\") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    points_xyzi: (N,4) [x,y,z,intensity]\n",
        "    Retourne les features utilisées pour le clustering HGP.\n",
        "    \"\"\"\n",
        "    x, y, z, i = points_xyzi[:,0], points_xyzi[:,1], points_xyzi[:,2], points_xyzi[:,3]\n",
        "    if mode == \"none\":\n",
        "        return points_xyzi[:, :3]               # x,y,z\n",
        "    if mode == \"bev_xy\":\n",
        "        return np.stack([x, y], axis=1)         # vue du dessus\n",
        "    if mode == \"bev_xyzi\":\n",
        "        return np.stack([x, y, i], axis=1)      # XY + intensité\n",
        "    if mode == \"polar\":\n",
        "        r = np.sqrt(x**2 + y**2)\n",
        "        theta = np.arctan2(y, x)\n",
        "        return np.stack([r, theta, z], axis=1)\n",
        "    raise ValueError(f\"Mode PREPROC inconnu: {mode}\")\n",
        "\n",
        "def assign_instances_from_clusters(cluster_ids: np.ndarray, class_ids: np.ndarray, thing_ids: list) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    - cluster_ids: labels de clustering (>0 = instance, -1/noise sera mis à 0)\n",
        "    - class_ids:   labels sémantiques bruts (SemanticKITTI) pour les points clusterisés, 0 sinon\n",
        "    - thing_ids:   liste des classes 'thing' (brutes)\n",
        "    Construit des IDs d'instance >0 uniques par scan, offsettés par classe.\n",
        "    \"\"\"\n",
        "    cluster_ids = np.asarray(cluster_ids).reshape(-1)\n",
        "    class_ids   = np.asarray(class_ids).reshape(-1)\n",
        "    inst = np.zeros_like(cluster_ids, dtype=np.int32)\n",
        "    base = {cid: 1000*(i+1) for i, cid in enumerate(thing_ids)}\n",
        "    for cid in np.unique(class_ids):\n",
        "        if cid == 0:\n",
        "            continue\n",
        "        mask = class_ids == cid\n",
        "        clusters = cluster_ids[mask]\n",
        "        uniq = [u for u in np.unique(clusters) if u > 0]\n",
        "        # réindexe en 1..K au sein de la classe\n",
        "        mapping = {u: j+1 for j,u in enumerate(sorted(uniq))}\n",
        "        out = np.array([mapping.get(v, 0) for v in clusters], dtype=np.int32)\n",
        "        inst[mask] = base[cid] + out\n",
        "    return inst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 5) Pipeline HGP: par scan et par classe (sans post-processing)\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_hgp_on_sequence(seq_id: str) -> bool:\n",
        "    seq_root = Path(DATA_ROOT) / \"sequences\" / seq_id\n",
        "    out_pred_dir = Path(OUT_ROOT) / \"sequences\" / seq_id / \"predictions\"\n",
        "    out_pred_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    vel_dir, label_dir, stems = kitti_scan_paths(seq_root)\n",
        "\n",
        "    # Charge sémantique brute (pour écriture panoptique finale)\n",
        "    sem_by_stem = {}\n",
        "    for stem in tqdm(stems, desc=f\"[{seq_id}] Prépare sémantique\"):\n",
        "        sem_by_stem[stem] = load_semantics_for_scan(seq_root, stem).astype(np.int32)\n",
        "\n",
        "    # Pour chaque scan, on clusterise indépendamment par classe 'thing'\n",
        "    for stem in tqdm(stems, desc=f\"[{seq_id}] Clustering HGP\"):\n",
        "        pts = read_points_bin(vel_dir / f\"{stem}.bin\")  # (N,4)\n",
        "        feats = compute_features(pts, PREPROC)          # (N,d)\n",
        "        sem_raw = sem_by_stem[stem]\n",
        "\n",
        "        N = feats.shape[0]\n",
        "        cluster_ids = np.zeros(N, dtype=np.int32)\n",
        "        class_ids   = np.zeros(N, dtype=np.int32)\n",
        "\n",
        "        for cid in THING_RAW_IDS:\n",
        "            mask = (sem_raw == cid)\n",
        "            n = int(mask.sum())\n",
        "            if n < min_cluster_size:\n",
        "                continue\n",
        "\n",
        "            X = feats[mask]\n",
        "\n",
        "            # Appel direct à HypergraphPercol (local)\n",
        "            labels = HypergraphPercol(\n",
        "                M=X,\n",
        "                K=K,\n",
        "                min_cluster_size=min_cluster_size,\n",
        "                min_samples=min_samples,\n",
        "                method=method,\n",
        "                splitting=splitting,\n",
        "                weight_face=weight_face,\n",
        "                label_all_points=label_all_points,\n",
        "                return_multi_clusters=return_multi_clusters,\n",
        "                complex_chosen=complex_chosen,\n",
        "                expZ=expZ,\n",
        "                cgal_root=str(repo_dir / \"CGALDelaunay\"),\n",
        "                verbeux=HGP_VERBOSE,\n",
        "            )\n",
        "            labels = np.asarray(labels).reshape(-1).astype(np.int32)\n",
        "            # hgp_clusterer renvoie souvent -1 pour le bruit\n",
        "            labels[labels < 0] = 0\n",
        "\n",
        "            # Injecte dans les tableaux globaux\n",
        "            cluster_ids[mask] = labels\n",
        "            class_ids[mask]   = cid\n",
        "\n",
        "        # Construit les IDs d'instances panoptiques finaux\n",
        "        inst = assign_instances_from_clusters(cluster_ids, class_ids, THING_RAW_IDS)\n",
        "\n",
        "        # Écrit panoptic .label (sémantique brute + instance >0 pour THINGS uniquement)\n",
        "        panoptic = pack_panoptic(sem_raw.astype(np.uint16), inst.astype(np.uint16))\n",
        "        (out_pred_dir / f\"{stem}.label\").write_bytes(panoptic.astype(np.uint32).tobytes())\n",
        "\n",
        "    return True\n",
        "\n",
        "# Boucle séquences\n",
        "for s in tqdm(SEQUENCES, desc=\"Séquences\"):\n",
        "    ok = run_hgp_on_sequence(s)\n",
        "    if not ok:\n",
        "        raise RuntimeError(f\"Échec sur séquence {s}\")\n",
        "\n",
        "print(\"Terminé. Prédictions écrites sous:\", OUT_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 6) Évaluation THINGS-only (optionnelle, via semantic-kitti-api)\n",
        "import os, sys, subprocess, json, re\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "EVAL_DIR = Path(\"/tmp/semkitti_eval\")\n",
        "API_DIR  = EVAL_DIR / \"semantic-kitti-api\"\n",
        "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Clone API if needed\n",
        "if not API_DIR.exists():\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/PRBonn/semantic-kitti-api.git\", str(API_DIR)], check=True)\n",
        "\n",
        "sys.path.insert(0, str(API_DIR))\n",
        "from evaluation.eval_pq import PanopticEval  # type: ignore\n",
        "\n",
        "# Split heuristique selon SEQUENCES\n",
        "split = \"valid\" if \"08\" in SEQUENCES else (\"test\" if any(s in (\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\") for s in SEQUENCES) else \"train\")\n",
        "\n",
        "# MAPPER et classes (comme au-dessus)\n",
        "MAPPER = {0: 0, 1: 0, 10: 1, 11: 2, 13: 5, 15: 3, 16: 5, 18: 4, 20: 5, 30: 6, 31: 7, 32: 8,\n",
        "          40: 9, 44: 10, 48: 11, 49: 12, 50: 13, 51: 14, 52: 0, 60: 9,\n",
        "          70: 15, 71: 16, 72: 17, 80: 18, 81: 19, 99: 0, 252: 1, 253: 7, 254: 6,\n",
        "          255: 8, 256: 5, 257: 5, 258: 4, 259: 5}\n",
        "STUFF_CLASS_IDS = [9,10,11,12,13,14,15,16,17,18,19]\n",
        "\n",
        "PHASE_SCENES = {\n",
        "    \"train\": [0,1,2,3,4,5,6,7,9,10],\n",
        "    \"valid\": [8],\n",
        "    \"val\":   [8],\n",
        "    \"test\":  [11,12,13,14,15,16,17,18,19,20,21],\n",
        "}\n",
        "\n",
        "def _frames(root, phase):\n",
        "    phase = \"val\" if phase == \"valid\" else phase\n",
        "    frames = []\n",
        "    for seq in PHASE_SCENES[phase]:\n",
        "        frames.extend(sorted(glob(os.path.join(root, \"sequences\", f\"{seq:02d}\", \"velodyne\", \"*.bin\"))))\n",
        "    return frames\n",
        "\n",
        "def _load_gt(bin_path):\n",
        "    u32 = np.fromfile(bin_path.replace(\"velodyne\",\"labels\")[:-3]+\"label\", dtype=np.uint32)\n",
        "    gt_sem_raw = u32 & 0xFFFF\n",
        "    gt_sem = np.vectorize(MAPPER.get)(gt_sem_raw).astype(np.int32)\n",
        "    gt_inst = u32 >> 16\n",
        "    return gt_sem, gt_inst\n",
        "\n",
        "def _load_pred(pred_root, bin_path):\n",
        "    rel = bin_path.split(\"sequences/\")[1]\n",
        "    stem = os.path.basename(rel)[:-4]\n",
        "    pred_file = os.path.join(pred_root, os.path.dirname(rel), \"predictions\", f\"{stem}.label\")\n",
        "    u32 = np.fromfile(pred_file, dtype=np.uint32)\n",
        "    pred_sem_raw = u32 & 0xFFFF\n",
        "    pred_sem = np.vectorize(MAPPER.get)(pred_sem_raw).astype(np.int32)\n",
        "    pred_inst = u32 >> 16\n",
        "    mask = np.isin(pred_sem, STUFF_CLASS_IDS + [0])\n",
        "    pred_inst = pred_inst.copy()\n",
        "    pred_inst[mask] = 0\n",
        "    return pred_sem, pred_inst\n",
        "\n",
        "frames = _frames(DATA_ROOT, split)\n",
        "ignore = [0] + STUFF_CLASS_IDS\n",
        "evaluator = PanopticEval(20, ignore=ignore, min_points=int(min_cluster_size))\n",
        "\n",
        "print(f\"Evaluating scans [{split}] — THINGS only | frames={len(frames)}\")\n",
        "for bin_path in frames:\n",
        "    gt_sem, gt_inst = _load_gt(bin_path)\n",
        "    pred_sem, pred_inst = _load_pred(OUT_ROOT, bin_path)\n",
        "    if pred_sem.shape[0] != gt_sem.shape[0]:\n",
        "        raise RuntimeError(f\"Point count mismatch for {bin_path}: pred={pred_sem.shape[0]} gt={gt_sem.shape[0]}\")\n",
        "    evaluator.addBatch(pred_sem, pred_inst, gt_sem, gt_inst)\n",
        "\n",
        "res = evaluator.getPQ()\n",
        "print(\"\\nRésultats panoptiques (THINGS):\")\n",
        "print(json.dumps(res, indent=2))\n",
        "\n",
        "# Résumé\n",
        "pq = res.get(\"All\", {})\n",
        "if isinstance(pq, dict):\n",
        "    pq_th = pq.get(\"pq\", None)\n",
        "    rq_th = pq.get(\"rq\", None)\n",
        "    sq_th = pq.get(\"sq\", None)\n",
        "    if pq_th is not None and rq_th is not None and sq_th is not None:\n",
        "        print(f\"\\nPQ_th : {pq_th:.3f} | RQ_th : {rq_th:.3f} | SQ_th : {sq_th:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 7) Visualisation 3D rapide (Plotly) — un scan\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from pathlib import Path\n",
        "\n",
        "def visualize_one(seq_id=\"08\", index=0, decimate=10):\n",
        "    seq_root = Path(DATA_ROOT) / \"sequences\" / seq_id\n",
        "    vel_dir, label_dir, stems = kitti_scan_paths(seq_root)\n",
        "\n",
        "    index = int(index) % len(stems)\n",
        "    stem = stems[index]\n",
        "\n",
        "    pts = read_points_bin(vel_dir / f\"{stem}.bin\")\n",
        "    pred32 = read_label_file(Path(OUT_ROOT) / \"sequences\" / seq_id / \"predictions\" / f\"{stem}.label\")\n",
        "\n",
        "    pred_sem = unpack_semantic(pred32)\n",
        "    pred_inst = (pred32.astype(np.uint32) >> 16).astype(np.int32)\n",
        "\n",
        "    N = pts.shape[0]\n",
        "    step = max(1, int(decimate))\n",
        "    idx = np.arange(0, N, step)\n",
        "\n",
        "    x, y, z = pts[idx,0], pts[idx,1], pts[idx,2]\n",
        "    inst = pred_inst[idx]\n",
        "\n",
        "    uniq = np.unique(inst)\n",
        "    lut = {0: '#808080'}\n",
        "    WARM = [\"#e6194b\",\"#3cb44b\",\"#ffe119\",\"#0082c8\",\"#f58231\",\"#911eb4\",\"#46f0f0\",\"#f032e6\",\"#d2f53c\",\n",
        "            \"#fabebe\",\"#008080\",\"#e6beff\",\"#aa6e28\",\"#fffac8\",\"#800000\",\"#aaffc3\",\"#808000\",\"#ffd8b1\",\"#000080\",\"#808080\"]\n",
        "    for u in uniq:\n",
        "        iu = int(u)\n",
        "        if iu == 0:\n",
        "            continue\n",
        "        # couleur stable par ID\n",
        "        rng = np.random.default_rng(iu + 12345)\n",
        "        lut[iu] = WARM[int(rng.integers(0, len(WARM)))]\n",
        "\n",
        "    color = np.array([lut[int(v)] for v in inst])\n",
        "\n",
        "    fig = go.Figure(data=[go.Scatter3d(\n",
        "        x=x, y=y, z=z, mode=\"markers\",\n",
        "        marker=dict(size=2, color=color, opacity=0.9),\n",
        "        showlegend=False\n",
        "    )])\n",
        "    fig.update_layout(width=900, height=700, title=f\"Prediction HGP — seq {seq_id} / frame {stem} (subsample 1/{step})\")\n",
        "    fig.show()\n",
        "\n",
        "# Exemple d'appel:\n",
        "# visualize_one(\"08\", 0, 10)"
      ]
    }
  ]
}