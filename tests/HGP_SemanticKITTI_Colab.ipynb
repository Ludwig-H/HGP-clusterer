{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "HGP_SemanticKITTI_local_minimal.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVuC21HpuG2F"
      },
      "source": [
        "# HGP clusterer sur SemanticKITTI (local, sans post-traitement)\n",
        "\n",
        "Ce notebook :\n",
        "\n",
        "- installe les dépendances système et Python nécessaires,\n",
        "- récupère **hgp_clusterer** depuis votre GitHub,\n",
        "- lance **HypergraphPercol** localement sur SemanticKITTI,\n",
        "- applique les mêmes *pré-traitements* que vos notebooks (`none`, `bev_xy`, `bev_xyzi`, `polar`),\n",
        "- **sans aucun post-traitement** (pas de KNN, pas de merges, pas de box splitting),\n",
        "- écrit les fichiers `.label` panoptiques au format SemanticKITTI,\n",
        "- propose une visualisation 3D simple d'un scan."
      ],
      "id": "eVuC21HpuG2F"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Colab: monter Drive si dispo\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "print(\"OK setup.\")\n"
      ],
      "metadata": {
        "id": "PQkIzWfUK4Ti"
      },
      "id": "PQkIzWfUK4Ti",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A4ZtYFfuG2H"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title 0) Dépendances système (CGAL, TBB, etc.)\n",
        "%%bash\n",
        "set -euo pipefail\n",
        "apt-get update -qq\n",
        "apt-get install -y -qq build-essential cmake libcgal-dev libtbb-dev libtbbmalloc2 \\\n",
        "  libgmp-dev libmpfr-dev libeigen3-dev"
      ],
      "id": "7A4ZtYFfuG2H"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUtAHNQSuG2I"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title 0-bis) Dépendances Python\n",
        "!pip -q install --upgrade pip setuptools wheel Cython cmake jedi\n",
        "!pip -q install numpy scipy scikit-learn plotly tqdm joblib"
      ],
      "id": "bUtAHNQSuG2I"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "mkdir -p \"${WORKDIR}\"\n",
        "cd \"${WORKDIR}\"\n",
        "if [ -d HGP-clusterer ]; then\n",
        "    git -C HGP-clusterer pull --ff-only\n",
        "else\n",
        "    git clone https://github.com/Ludwig-H/HGP-clusterer.git\n",
        "fi\n",
        "if [ -d cyminiball ]; then\n",
        "    git -C cyminiball pull --ff-only\n",
        "else\n",
        "    git clone https://github.com/Ludwig-H/cyminiball.git\n",
        "fi\n"
      ],
      "metadata": {
        "id": "sjWmCvEUun7d"
      },
      "id": "sjWmCvEUun7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "mkdir -p \"${WORKDIR}/wheels\"\n",
        "cd \"${WORKDIR}/cyminiball\"\n",
        "python3 -m pip wheel --no-build-isolation --no-deps --wheel-dir=\"${WORKDIR}/wheels\" .\n",
        "python3 -m pip install --force-reinstall --no-deps --no-index --find-links=\"${WORKDIR}/wheels\" cyminiball\n",
        "# Le \"--no-deps\" indispensable pour que numpy ne se télécharge pas en version 2.3.4, créant des problèmes de compatibilité...\n"
      ],
      "metadata": {
        "id": "lUh3OfPHusWO"
      },
      "id": "lUh3OfPHusWO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "cd \"${WORKDIR}/HGP-clusterer\"\n",
        "python3 scripts/setup_cgal.py\n"
      ],
      "metadata": {
        "id": "fLjpz5z7uvd-"
      },
      "id": "fLjpz5z7uvd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "cd \"${WORKDIR}/HGP-clusterer/CGALDelaunay\"\n",
        "\n",
        "projects=(\n",
        "    EdgesCGALDelaunay2D\n",
        "    EdgesCGALDelaunay3D\n",
        "    EdgesCGALDelaunayND\n",
        "    EdgesCGALWeightedDelaunay2D\n",
        "    EdgesCGALWeightedDelaunay3D\n",
        "    EdgesCGALWeightedDelaunayND\n",
        ")\n",
        "\n",
        "for project in \"${projects[@]}\"; do\n",
        "    cmake -S \"${project}\" -B \"${project}/build\" -DCMAKE_BUILD_TYPE=Release\n",
        "    cmake --build \"${project}/build\" --config Release\n",
        "    cmake --install \"${project}/build\" --prefix \"${WORKDIR}/HGP-clusterer\"\n",
        "done\n"
      ],
      "metadata": {
        "id": "y6Y6VKuIuxV1"
      },
      "id": "y6Y6VKuIuxV1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "WORKDIR=\"${HGP_WORKDIR:-/content}\"\n",
        "cd \"${WORKDIR}/HGP-clusterer\"\n",
        "python3 -m pip install -v --no-deps .\n"
      ],
      "metadata": {
        "id": "wsuClpWqu19u"
      },
      "id": "wsuClpWqu19u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "workdir = os.environ.get(\"HGP_WORKDIR\", \"/content\")\n",
        "repo_root = os.path.join(workdir, \"HGP-clusterer\")\n",
        "os.environ[\"CGALDELAUNAY_ROOT\"] = os.path.join(repo_root, \"CGALDelaunay\")\n",
        "\n",
        "from hgp_clusterer import HypergraphPercol"
      ],
      "metadata": {
        "id": "ha6MJoCPu2at"
      },
      "id": "ha6MJoCPu2at",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lyy1GKBuG2J"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title 1) Récupération et installation de hgp_clusterer (GitHub)\n",
        "import os, subprocess, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# === Paramètre clé : URL du dépôt Git de hgp_clusterer ===\n",
        "# Remplacez par l'URL de votre dépôt si besoin.\n",
        "REPO_URL = os.environ.get(\"HGP_REPO_URL\", \"https://github.com/Ludwig-H/HGP-clusterer.git\")\n",
        "\n",
        "WORKDIR = Path(os.environ.get(\"HGP_WORKDIR\", \"/content\")).resolve()\n",
        "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_dir = WORKDIR / \"HGP-clusterer\"\n",
        "cymini_dir = WORKDIR / \"cyminiball\"\n",
        "\n",
        "def _run(cmd, **kw):\n",
        "    print(\"+\", cmd)\n",
        "    subprocess.run(cmd, check=True, **kw)\n",
        "\n",
        "# Clone / update hgp_clusterer\n",
        "if repo_dir.exists():\n",
        "    _run([\"git\", \"-C\", str(repo_dir), \"pull\", \"--ff-only\"])\n",
        "else:\n",
        "    _run([\"git\", \"clone\", REPO_URL, str(repo_dir)])\n",
        "\n",
        "# cyminiball (optionnel mais recommandé pour certains backends)\n",
        "if cymini_dir.exists():\n",
        "    _run([\"git\", \"-C\", str(cymini_dir), \"pull\", \"--ff-only\"])\n",
        "else:\n",
        "    _run([\"git\", \"clone\", \"https://github.com/Ludwig-H/cyminiball.git\", str(cymini_dir)])\n",
        "\n",
        "wheels = WORKDIR / \"wheels\"\n",
        "wheels.mkdir(parents=True, exist_ok=True)\n",
        "_run([sys.executable, \"-m\", \"pip\", \"wheel\", \"--no-build-isolation\", \"--no-deps\", \"--wheel-dir\", str(wheels), str(cymini_dir)])\n",
        "_run([sys.executable, \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"--no-deps\", \"--no-index\", f\"--find-links={wheels}\", \"cyminiball\"])\n",
        "\n",
        "# Installation de hgp_clusterer lui-même (laisse le setup gérer les extensions)\n",
        "_run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(repo_dir)])\n",
        "\n",
        "# Variable d'environnement pour les exécutables CGALDelaunay éventuels\n",
        "cgal_root = repo_dir / \"CGALDelaunay\"\n",
        "os.environ[\"CGALDELAUNAY_ROOT\"] = str(cgal_root)\n",
        "print(\"CGALDELAUNAY_ROOT =\", os.environ[\"CGALDELAUNAY_ROOT\"])\n",
        "print(\"Dépôt hgp_clusterer prêt:\", repo_dir)"
      ],
      "id": "2lyy1GKBuG2J"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "workdir = os.environ.get(\"HGP_WORKDIR\", \"/content\")\n",
        "repo_root = os.path.join(workdir, \"HGP-clusterer\")\n",
        "\n",
        "# Add repo_root to sys.path to ensure subprocesses can find hgp_clusterer\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "os.environ[\"CGALDELAUNAY_ROOT\"] = os.path.join(repo_root, \"CGALDelaunay\")\n",
        "\n",
        "from hgp_clusterer import HypergraphPercol"
      ],
      "metadata": {
        "id": "FQnXFgy3DhRm"
      },
      "id": "FQnXFgy3DhRm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7roChIsfuG2J"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title 2) Imports + configuration centrale\n",
        "\n",
        "import os, json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "# from hgp_clusterer import HypergraphPercol\n",
        "\n",
        "\n",
        "\n",
        "# === Dossiers ===\n",
        "DATA_ROOT = \"/content/drive/MyDrive/Datasets/semantic_kitti\"  #@param {type:\"string\"}\n",
        "# Sémantique pour le clustering: \"oracle\" (vérité terrain), \"waffleiron\" (preds), \"custom\" (vos propres prédictions)\n",
        "SEMANTICS_SOURCE = \"oracle\"  #@param [\"oracle\", \"waffleiron\", \"custom\"]\n",
        "# Si \"waffleiron\" ou \"custom\": racine contenant sequences/*/predictions/*.{label,labels,npy,npz}\n",
        "PRED_ROOT = \"/content/drive/MyDrive/Datasets/semantic_kitti/WaffleIron\"  #@param {type:\"string\"}\n",
        "\n",
        "# === Séquences à traiter ===\n",
        "SEQUENCES = [\"08\"]  #@param {type:\"raw\"}\n",
        "\n",
        "# === Paramètres HGP ===\n",
        "K = 2  #@param {type:\"integer\"}\n",
        "min_cluster_size = 2  #@param {type:\"integer\"}\n",
        "min_samples = K + 1    #@param {type:\"raw\"}\n",
        "method = \"DBSCAN\"         #@param [\"eom\",\"leaf\", \"DBSCAN\"]\n",
        "splitting = True      #@param {type:\"boolean\"}\n",
        "weight_face = \"lambda\" #@param [\"lambda\",\"uniform\",\"unique\"]\n",
        "label_all_points = False  #@param {type:\"boolean\"}\n",
        "return_multi_clusters = False  #@param {type:\"boolean\"}\n",
        "complex_chosen = \"orderk_delaunay\"  #@param [\"orderk_delaunay\",\"delaunay\",\"weighted_delaunay\"]\n",
        "expZ = 1  #@param {type:\"integer\"}\n",
        "HGP_VERBOSE = False  #@param {type:\"boolean\"}\n",
        "\n",
        "# === Pré-traitements (sans post-processing) ===\n",
        "PREPROC = \"none\"  #@param [\"none\",\"bev_xy\",\"bev_xyzi\",\"polar\"]\n",
        "\n",
        "# === Sortie: structure SemanticKITTI ===\n",
        "RUN_NAME = f\"HGP-K{K}_min{min_cluster_size}_expZ{expZ}_pre{PREPROC}_method{method}\"\n",
        "OUT_ROOT = f\"/content/drive/MyDrive/Datasets/semantic_kitti/experiments_semkitti/{RUN_NAME}\"\n",
        "Path(OUT_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "print(\"Sortie:\", OUT_ROOT)\n",
        "\n",
        "# === Classes thing/stuff brutes SemanticKITTI ===\n",
        "THING_RAW_IDS = [10,11,15,18,20,30,31,32]  # car, bicycle, motorcycle, truck, other-vehicle, person, bicyclist, motorcyclist\n",
        "STUFF_RAW_IDS = [40,44,48,49,50,51,70,71,72,80,81]\n",
        "\n",
        "print(\"Config chargée.\")"
      ],
      "id": "7roChIsfuG2J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-U6DnbzuG2K"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title 3) Utilitaires I/O (KITTI) + encodage panoptique\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def kitti_scan_paths(seq_dir: Path):\n",
        "    vel_dir = seq_dir / \"velodyne\"\n",
        "    label_dir = seq_dir / \"labels\"\n",
        "    assert vel_dir.is_dir(), f\"Manque velodyne sous {seq_dir}\"\n",
        "    stems = sorted([p.stem for p in vel_dir.glob(\"*.bin\")])\n",
        "    return vel_dir, label_dir, stems\n",
        "\n",
        "def read_points_bin(bin_path: Path) -> np.ndarray:\n",
        "    arr = np.fromfile(str(bin_path), dtype=np.float32)\n",
        "    return arr.reshape(-1, 4)  # x,y,z,remission\n",
        "\n",
        "def read_label_file(label_path: Path) -> np.ndarray:\n",
        "    # 32-bit uint: upper 16 bits = instance id, lower 16 = semantic id\n",
        "    return np.fromfile(str(label_path), dtype=np.uint32)\n",
        "\n",
        "def pack_panoptic(semantic: np.ndarray, instance: np.ndarray) -> np.ndarray:\n",
        "    assert semantic.shape == instance.shape\n",
        "    return ((instance.astype(np.uint32) << 16) | (semantic.astype(np.uint32)))\n",
        "\n",
        "def unpack_semantic(label32: np.ndarray) -> np.ndarray:\n",
        "    return (label32 & 0xFFFF).astype(np.uint16)\n",
        "\n",
        "def _find_pred_file(pred_dir: Path, stem: str) -> Optional[Path]:\n",
        "    # Supporte .label, .labels, .npy, .npz\n",
        "    for ext in (\".label\", \".labels\", \".npy\", \".npz\"):\n",
        "        p = pred_dir / f\"{stem}{ext}\"\n",
        "        if p.is_file():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def _load_semantics_generic(pred_path: Path) -> np.ndarray:\n",
        "    if pred_path.suffix in (\".label\", \".labels\"):\n",
        "        u32 = np.fromfile(str(pred_path), dtype=np.uint32)\n",
        "        return (u32 & 0xFFFF).astype(np.uint16)\n",
        "    if pred_path.suffix == \".npy\":\n",
        "        arr = np.load(str(pred_path))\n",
        "        arr = np.array(arr).reshape(-1)\n",
        "        return arr.astype(np.uint16)\n",
        "    if pred_path.suffix == \".npz\":\n",
        "        data = np.load(str(pred_path))\n",
        "        key = list(data.keys())[0]\n",
        "        arr = np.array(data[key]).reshape(-1)\n",
        "        return arr.astype(np.uint16)\n",
        "    raise ValueError(f\"Extension non supportée: {pred_path}\")\n",
        "\n",
        "def load_semantics_for_scan(seq_root: Path, stem: str) -> np.ndarray:\n",
        "    if SEMANTICS_SOURCE == \"oracle\":\n",
        "        _, label_dir, _ = kitti_scan_paths(seq_root)\n",
        "        gt32 = read_label_file(label_dir / f\"{stem}.label\")\n",
        "        return unpack_semantic(gt32)\n",
        "    else:\n",
        "        pred_dir = Path(PRED_ROOT) / \"sequences\" / seq_root.name / \"predictions\"\n",
        "        pred_file = _find_pred_file(pred_dir, stem)\n",
        "        assert pred_file is not None, f\"Prediction manquante: {pred_dir}/{stem}.*\"\n",
        "        return _load_semantics_generic(pred_file)\n",
        "\n",
        "print(\"I/O utils chargés.\")"
      ],
      "id": "a-U6DnbzuG2K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8yRjGsYuG2K"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title 4) Pré-traitements (features) + assignation d'instances\n",
        "import numpy as np\n",
        "\n",
        "def compute_features(points_xyzi: np.ndarray, mode: str = \"none\") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    points_xyzi: (N,4) [x,y,z,intensity]\n",
        "    Retourne les features utilisées pour le clustering HGP.\n",
        "    \"\"\"\n",
        "    x, y, z, i = points_xyzi[:,0], points_xyzi[:,1], points_xyzi[:,2], points_xyzi[:,3]\n",
        "    if mode == \"none\":\n",
        "        return points_xyzi[:, :3]               # x,y,z\n",
        "    if mode == \"bev_xy\":\n",
        "        return np.stack([x, y], axis=1)         # vue du dessus\n",
        "    if mode == \"bev_xyzi\":\n",
        "        return np.stack([x, y, i], axis=1)      # XY + intensité\n",
        "    if mode == \"polar\":\n",
        "        r = np.sqrt(x**2 + y**2)\n",
        "        theta = np.arctan2(y, x)\n",
        "        return np.stack([r, theta, z], axis=1)\n",
        "    raise ValueError(f\"Mode PREPROC inconnu: {mode}\")\n",
        "\n",
        "def assign_instances_from_clusters(cluster_ids: np.ndarray, class_ids: np.ndarray, thing_ids: list) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    - cluster_ids: labels de clustering (>0 = instance, -1 ou 0 = bruit)\n",
        "    - class_ids:   labels sémantiques bruts (SemanticKITTI) pour les points clusterisés, 0 sinon\n",
        "    - thing_ids:   liste des classes 'thing' (brutes)\n",
        "    Construit des IDs d'instance >0 uniques par scan, offsettés par classe.\n",
        "    \"\"\"\n",
        "    cluster_ids = np.asarray(cluster_ids).reshape(-1)\n",
        "    class_ids   = np.asarray(class_ids).reshape(-1)\n",
        "    inst = np.zeros_like(cluster_ids, dtype=np.int32)\n",
        "    base = {cid: 1000*(i+1) for i, cid in enumerate(thing_ids)}\n",
        "    for cid in np.unique(class_ids):\n",
        "        if cid == 0:\n",
        "            continue\n",
        "        mask = class_ids == cid\n",
        "        clusters = cluster_ids[mask]\n",
        "        uniq = [u for u in np.unique(clusters) if u > 0]\n",
        "        mapping = {u: j+1 for j,u in enumerate(sorted(uniq))}\n",
        "        out = np.array([mapping.get(v, 0) for v in clusters], dtype=np.int32)\n",
        "        inst[mask] = base[cid] + out\n",
        "    return inst\n",
        "\n",
        "print(\"Pré-traitements + assignation prêts.\")"
      ],
      "id": "t8yRjGsYuG2K"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "#@title 0) Extraire 08.zip depuis Drive vers /tmp (exécution bash)\n",
        "# Requiert que Drive soit monté avant:\n",
        "# from google.colab import drive; drive.mount('/content/drive')\n",
        "set -euo pipefail\n",
        "\n",
        "ZIP=\"/content/drive/MyDrive/Datasets/semantic_kitti/sequences/08.zip\"\n",
        "DEST=\"/tmp/semkitti_local/sequences\"\n",
        "\n",
        "if [ ! -f \"$ZIP\" ]; then\n",
        "  echo \"Introuvable: $ZIP\"\n",
        "  echo \"Vérifie le chemin (ex: /content/drive/MyDrive/sequences/08.zip) et que Drive est monté.\"\n",
        "  exit 1\n",
        "fi\n",
        "\n",
        "mkdir -p \"$DEST\"\n",
        "echo \"Extraction de $ZIP -> $DEST\"\n",
        "unzip -q -o \"$ZIP\" -d \"$DEST\"\n",
        "echo \"Contenu extrait:\"\n",
        "find \"$DEST/08\" -maxdepth 2 -type d -print\n"
      ],
      "metadata": {
        "id": "z12ebJgS-ux_"
      },
      "id": "z12ebJgS-ux_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 0b) Utiliser la séquence locale extraite\n",
        "from pathlib import Path\n",
        "\n",
        "# On travaille sur /tmp/semkitti_local/sequences/08 extrait ci-dessus\n",
        "DATA_ROOT = \"/tmp/semkitti_local\"\n",
        "\n",
        "# Optionnel: vérification rapide\n",
        "assert (Path(DATA_ROOT) / \"sequences\" / \"08\" / \"velodyne\").exists(), \"velodyne introuvable sous /tmp/semkitti_local/sequences/08\"\n",
        "assert (Path(DATA_ROOT) / \"sequences\" / \"08\" / \"labels\").exists(), \"labels introuvable sous /tmp/semkitti_local/sequences/08\"\n",
        "\n",
        "print(\"DATA_ROOT =\", DATA_ROOT, \"| SEQUENCES =\", SEQUENCES)\n"
      ],
      "metadata": {
        "id": "AYoCHO01-yOm"
      },
      "id": "AYoCHO01-yOm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/HGP-clusterer\n",
        "\n",
        "import hgp_clusterer\n",
        "from hgp_clusterer import HypergraphPercol\n"
      ],
      "metadata": {
        "id": "PipOyTwgIMsv"
      },
      "id": "PipOyTwgIMsv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import subprocess, shlex, os, shutil, zipfile, uuid\n",
        "\n",
        "# ---------- utilitaires I/O ----------\n",
        "\n",
        "def _rsync_dir(src: str, dst: str):\n",
        "    Path(dst).mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        cmd = f'rsync -a --info=progress2 \"{src.rstrip(\"/\")}/\" \"{dst.rstrip(\"/\")}/\"'\n",
        "        subprocess.run([\"bash\", \"-lc\", cmd], check=True)\n",
        "    except subprocess.CalledProcessError:\n",
        "        for p in Path(src).glob(\"*\"):\n",
        "            q = Path(dst) / p.name\n",
        "            if p.is_dir():\n",
        "                shutil.copytree(p, q, dirs_exist_ok=True)\n",
        "            else:\n",
        "                shutil.copy2(p, q)\n",
        "\n",
        "def ensure_local_sequence(seq_id: str, src_root: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Si DATA_ROOT est distant (Drive/gcsfuse/NFS/gs://), on synchronise une fois vers /tmp\n",
        "    pour des lectures rapides.\n",
        "    \"\"\"\n",
        "    src_str = str(src_root)\n",
        "    local_root = Path(\"/tmp/semkitti_local\") / \"sequences\" / seq_id\n",
        "    if (local_root / \"labels\").exists() and (local_root / \"velodyne\").exists():\n",
        "        return local_root\n",
        "\n",
        "    is_remote = src_str.startswith(\"gs://\") or \"/content/drive\" in src_str or \"/mnt/\" in src_str\n",
        "    if not is_remote:\n",
        "        return src_root\n",
        "\n",
        "    print(f\"[{seq_id}] Staging local -> {local_root}\")\n",
        "    (local_root / \"labels\").mkdir(parents=True, exist_ok=True)\n",
        "    (local_root / \"velodyne\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if src_str.startswith(\"gs://\"):\n",
        "        for sub in [\"labels\", \"velodyne\"]:\n",
        "            cmd = f'gsutil -m rsync -r \"{src_str}/{sub}\" \"{(local_root / sub).as_posix()}\"'\n",
        "            subprocess.run([\"bash\", \"-lc\", cmd], check=True)\n",
        "    else:\n",
        "        _rsync_dir(f\"{src_str}/labels\", (local_root / \"labels\").as_posix())\n",
        "        _rsync_dir(f\"{src_str}/velodyne\", (local_root / \"velodyne\").as_posix())\n",
        "\n",
        "    return local_root\n",
        "\n",
        "# ---------- sémantique / instance helpers ----------\n",
        "\n",
        "def fast_load_semantics_for_scan(seq_root: Path, stem: str) -> np.ndarray:\n",
        "    # Lecture ultra simple: panoptic uint32 -> sem = val & 0xFFFF\n",
        "    arr = np.fromfile((seq_root / \"labels\" / f\"{stem}.label\").as_posix(), dtype=np.uint32)\n",
        "    return (arr & 0xFFFF).astype(np.int32)"
      ],
      "metadata": {
        "id": "avg5Jj97_HA_"
      },
      "id": "avg5Jj97_HA_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPM4LZ7quG2K"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from hgp_clusterer import HypergraphPercol\n",
        "\n",
        "import numpy as np\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "\n",
        "# ------------------------ MAPPINGS & CONSTANTES ------------------------------\n",
        "BBOX_WEB = {1: [3., 1.8, 1.5], # car: https://www.motor1.com/news/707996/vehicles-larger-than-ever-usa-europe\n",
        "            2: [1.75, 0.61, 1.3], # bicycle: https://thebestbikelock.com/wp-content/uploads/2020/01/one-bike-average-size.gif\n",
        "            3: [2., 0.95, 1.3], # motorcycle: https://carparkjourney.wordpress.com/2013/07/16/what-is-the-average-size-of-a-motorbike/\n",
        "            4: [8, 3, 1.5], # truck\n",
        "            5: [8, 3, 1.5], # other-vehicle\n",
        "            6: [0.94, 0.94, 1.5], # person: RLSP arm span height: https://pubmed.ncbi.nlm.nih.gov/25063245/  average height in germany https://en.wikipedia.org/wiki/Average_human_height_by_country 179. We get 179*1.06/2\n",
        "            7: [1.75, 0.61, 1.5], # bicyclist: bicycle\n",
        "            8: [2.1, 0.95, 1.5], # motorcyclist: motorcycle\n",
        "            }\n",
        "\n",
        "LEARNING_MAP_INVERSE = {0:0,1:10,2:11,3:15,4:18,5:20,6:30,7:31,8:32,9:40,10:44,11:48,12:49,13:50,14:51,15:70,16:71,17:72,18:80,19:81}\n",
        "mapper = {0:0,1:0,10:1,11:2,13:5,15:3,16:5,18:4,20:5,30:6,31:7,32:8,40:9,44:10,48:11,49:12,50:13,51:14,52:0,60:9,70:15,71:16,72:17,80:18,81:19,99:0,252:1,253:7,254:6,255:8,256:5,257:5,258:4,259:5}\n",
        "\n",
        "# # --------------------------- OUTILS GEOMETRIQUES -----------------------------\n",
        "\n",
        "def convex_hull_measure(points: np.ndarray) -> float:\n",
        "    \"\"\"Compute the area (2D) or volume (3D) of the convex hull.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    points : array-like, shape (n_samples, 2) or (n_samples, 3)\n",
        "        Nuage de points en 2D ou 3D. Chaque ligne correspond à un point.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Aire (2D) ou volume (3D) de l'enveloppe convexe.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        Si la dimension n'est pas 2 ou 3, ou s'il n'y a pas assez de points.\n",
        "    \"\"\"\n",
        "\n",
        "    pts = np.asarray(points, dtype=float)\n",
        "\n",
        "    if pts.ndim != 2:\n",
        "        raise ValueError(\n",
        "            f\"`points` doit être un tableau 2D (n_samples, dim), pas {pts.ndim}D.\"\n",
        "        )\n",
        "\n",
        "    n_samples, dim = pts.shape\n",
        "\n",
        "    if dim not in (2, 3):\n",
        "        raise ValueError(\n",
        "            f\"La dimension doit être 2 ou 3 (reçu dim={dim}).\"\n",
        "        )\n",
        "\n",
        "    if n_samples <= dim:\n",
        "        return 0\n",
        "        # raise ValueError(\n",
        "        #     f\"Il faut au moins dim+1 points (dim={dim} => min={dim+1}), \"\n",
        "        #     f\"reçu n={n_samples}.\"\n",
        "        # )\n",
        "\n",
        "    # `volume` est :\n",
        "    #   - l'aire pour un problème 2D\n",
        "    #   - le volume pour un problème 3D\n",
        "    # QJ = 'joggle' pour gérer les cas quasi-dégénérés.\n",
        "    hull = ConvexHull(pts, qhull_options=\"QJ\")\n",
        "    return float(hull.volume)\n",
        "\n",
        "def loss_volume(points, volume_attendu) :\n",
        "    volume_calcule = convex_hull_measure(points)\n",
        "    return np.abs(volume_calcule - volume_attendu)\n",
        "\n",
        "\n",
        "def run_hgp_on_sequence(seq_id: str) -> bool:\n",
        "    seq_root = Path(DATA_ROOT) / \"sequences\" / seq_id\n",
        "    out_pred_dir = Path(OUT_ROOT) / \"sequences\" / seq_id / \"predictions\"\n",
        "    out_pred_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    vel_dir, label_dir, stems = kitti_scan_paths(seq_root)\n",
        "\n",
        "    # # Charge sémantique brute (pour écriture panoptique finale)\n",
        "    # sem_by_stem = {}\n",
        "    # for stem in tqdm(stems, desc=f\"[{seq_id}] Prépare sémantique\"):\n",
        "    #     sem_by_stem[stem] = load_semantics_for_scan(seq_root, stem).astype(np.int32)\n",
        "    # 1) Charger sémantiques + créer buffers d'instances (parallèle I/O)\n",
        "    sem_by_stem = {}\n",
        "    inst_by_stem = {}\n",
        "\n",
        "    def _load_one(stem):\n",
        "        sem = fast_load_semantics_for_scan(seq_root, stem)\n",
        "        return stem, sem\n",
        "\n",
        "    max_workers = os.cpu_count() # int(os.environ.get(\"SEM_LOAD_WORKERS\", \"16\"))\n",
        "    print(f\"[{seq_id}] Chargement sémantique (max_workers={max_workers})\")\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        for stem, sem in tqdm(ex.map(_load_one, stems),\n",
        "                              total=len(stems),\n",
        "                              desc=f\"[{seq_id}] Chargement sémantique\",\n",
        "                              ncols=0, leave=False):\n",
        "            sem_by_stem[stem] = sem\n",
        "            inst_by_stem[stem] = np.zeros_like(sem, dtype=np.int32)\n",
        "\n",
        "    # Pour chaque scan, on clusterise indépendamment par classe 'thing'\n",
        "    for stem in tqdm(stems, desc=f\"[{seq_id}] Clustering HGP\"):\n",
        "        pts = read_points_bin(vel_dir / f\"{stem}.bin\")  # (N,4)\n",
        "        feats = compute_features(pts, PREPROC)          # (N,d)\n",
        "        sem_raw = sem_by_stem[stem]\n",
        "\n",
        "        N = feats.shape[0]\n",
        "        cluster_ids = np.zeros(N, dtype=np.int32)\n",
        "        class_ids   = np.zeros(N, dtype=np.int32)\n",
        "\n",
        "        for cid in THING_RAW_IDS:\n",
        "            mask = (sem_raw == cid)\n",
        "            n = int(mask.sum())\n",
        "            if n < min_cluster_size:\n",
        "                continue\n",
        "\n",
        "            X = feats[mask]\n",
        "\n",
        "            # Create a local variable for the method parameter to avoid UnboundLocalError\n",
        "            hgp_method_param = method\n",
        "\n",
        "            if splitting :\n",
        "                cid_map = mapper[cid]\n",
        "                bbox = BBOX_WEB[cid_map]\n",
        "                d = X.shape[1]\n",
        "                if d == 2 :\n",
        "                    volume_attendu = bbox[0]*bbox[1]\n",
        "                elif d == 3 :\n",
        "                    volume_attendu = bbox[0]*bbox[1]*bbox[2]\n",
        "                else :\n",
        "                    raise ValueError(f\"Dimension non supportée: {d}\")\n",
        "                loss = lambda points : loss_volume(points, volume_attendu)\n",
        "            else :\n",
        "                loss = None\n",
        "            if hgp_method_param == \"DBSCAN\" :\n",
        "                hgp_method_param = bbox[0]/2\n",
        "            # print(\"Appel à HypergraphPercol\")\n",
        "            labels = HypergraphPercol(\n",
        "                M=X,\n",
        "                K=K,\n",
        "                min_cluster_size=min_cluster_size,\n",
        "                # min_samples=min_samples,\n",
        "                method=hgp_method_param, # Use the local parameter here\n",
        "                splitting=loss,\n",
        "                # weight_face=weight_face,\n",
        "                label_all_points=label_all_points,\n",
        "                # return_multi_clusters=return_multi_clusters,\n",
        "                # complex_chosen=complex_chosen,\n",
        "                expZ=expZ,\n",
        "                cgal_root=os.environ.get(\"CGALDELAUNAY_ROOT\", None),\n",
        "                verbeux=False,\n",
        "            )+1\n",
        "            labels = np.asarray(labels).reshape(-1).astype(np.int32)\n",
        "            labels[labels < 0] = 0\n",
        "\n",
        "            cluster_ids[mask] = labels\n",
        "            class_ids[mask]   = cid\n",
        "\n",
        "        inst = assign_instances_from_clusters(cluster_ids, class_ids, THING_RAW_IDS)\n",
        "\n",
        "        panoptic = pack_panoptic(sem_raw.astype(np.uint16), inst.astype(np.uint16))\n",
        "        (out_pred_dir / f\"{stem}.label\").write_bytes(panoptic.astype(np.uint32).tobytes())\n",
        "\n",
        "    return True\n",
        "\n",
        "for s in tqdm(SEQUENCES, desc=\"Séquences\"):\n",
        "    ok = run_hgp_on_sequence(s)\n",
        "    if not ok:\n",
        "        raise RuntimeError(f\"Échec sur séquence {s}\")\n",
        "\n",
        "print(\"Terminé. Prédictions écrites sous:\", OUT_ROOT)\n"
      ],
      "id": "FPM4LZ7quG2K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqZXSsPEuG2L"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 7) Visualisation 3D (Plotly): oracle vs HGP\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from pathlib import Path\n",
        "\n",
        "# Groupes SemanticKITTI\n",
        "THINGS_IDS = {10, 11, 15, 18, 20, 30, 31, 32}\n",
        "STUFF_IDS  = {40, 44, 48, 49, 50, 51, 70, 71, 72, 80, 81}\n",
        "\n",
        "# === Classes things SemanticKITTI ===\n",
        "# D'après l'extension panoptique: car, truck, other-vehicle, motorcycle, bicycle, person, bicyclist, motorcyclist\n",
        "THING_CLASS_IDS = [10, 18, 20, 15, 11, 30, 31, 32]\n",
        "\n",
        "CLASS_ID_TO_NAME = {\n",
        "    10: \"car\",\n",
        "    11: \"bicycle\",\n",
        "    15: \"motorcycle\",\n",
        "    18: \"truck\",\n",
        "    20: \"other-vehicle\",\n",
        "    30: \"person\",\n",
        "    31: \"bicyclist\",\n",
        "    32: \"motorcyclist\",\n",
        "}\n",
        "\n",
        "# Ids \"stuff\" courants (utiles pour la visualisation; non exhaustif)\n",
        "STUFF_CLASS_IDS = [40, 44, 48, 49, 50, 51, 52, 60, 70, 71, 72, 80]  # road, parking, sidewalk, other-ground, building, fence, vegetation, trunk, terrain, pole, traffic-sign, etc.\n",
        "CLASS_COLOR = {\n",
        "    10: '#F59664', 11: '#FFDE59', 15: '#E04F5F', 18: '#7F7F7F',\n",
        "    20: '#CFCFCF', 30: '#FF7F7F', 31: '#FFBF80', 32: '#FF8080',\n",
        "    40: '#808080', 44:'#B0B0B0', 48:'#C0C0C0', 49:'#A0A0A0',\n",
        "    50:'#8F8FBD', 51:'#7F7F7F', 52:'#5FA15F', 60:'#7B5E57',\n",
        "    70:'#9C9C5C', 71:'#FFD700', 72:'#A0A0FF', 80:'#B0E0E6'\n",
        "}\n",
        "# Noms de classes demandés (THINGS)\n",
        "CLASS_ID_TO_NAME = {\n",
        "    10: \"car\",\n",
        "    11: \"bicycle\",\n",
        "    15: \"motorcycle\",\n",
        "    18: \"truck\",\n",
        "    20: \"other-vehicle\",\n",
        "    30: \"person\",\n",
        "    31: \"bicyclist\",\n",
        "    32: \"motorcyclist\",\n",
        "}\n",
        "\n",
        "# Palette chaude pour THINGS, sombre pour STUFF\n",
        "WARM = [\n",
        "    \"#d73027\", \"#e6550d\", \"#e34a33\", \"#f46d43\", \"#fdae61\",\n",
        "    \"#f16913\", \"#fb6a4a\", \"#dd1c77\", \"#e31a1c\", \"#ff9f1c\",\n",
        "]\n",
        "DARK_STUFF = {\n",
        "    40: \"#222222\",  # road\n",
        "    44: \"#2a2a2a\",  # parking\n",
        "    48: \"#333333\",  # sidewalk\n",
        "    49: \"#1f1f1f\",  # other-ground\n",
        "    50: \"#2f3b4b\",  # building\n",
        "    51: \"#2c2c3c\",  # fence\n",
        "    70: \"#284b2f\",  # vegetation\n",
        "    71: \"#3d2b1f\",  # trunk\n",
        "    72: \"#2e3d1f\",  # terrain\n",
        "    80: \"#3a3a3a\",  # pole\n",
        "    81: \"#4c3a1e\",  # traffic-sign\n",
        "}\n",
        "STUFF_OPACITY = 0.28\n",
        "THING_OPACITY = 0.90\n",
        "def unpack_semantic(label32: np.ndarray) -> np.ndarray:\n",
        "    return (label32 & 0xFFFF).astype(np.uint16)\n",
        "\n",
        "def unpack_instance(label32: np.ndarray) -> np.ndarray:\n",
        "    return (label32 >> 16).astype(np.uint16)\n",
        "\n",
        "def load_panoptic_pred(seq_id: str, stem: str) -> np.ndarray:\n",
        "    pred_path = Path(OUT_ROOT) / \"sequences\" / seq_id / \"predictions\" / f\"{stem}.label\"\n",
        "    if pred_path.is_file():\n",
        "        return read_label_file(pred_path)\n",
        "    raise FileNotFoundError(str(pred_path))\n",
        "\n",
        "\n",
        "def make_colors_for_instances(inst_ids: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Une couleur par instance: 0 -> gris, >0 -> *palette chaude* stable par ID.\n",
        "    Conserve le nom de fonction de ton notebook.\n",
        "    \"\"\"\n",
        "    uniq = np.unique(inst_ids)\n",
        "    lut = {0: '#808080'}\n",
        "    for u in uniq:\n",
        "        iu = int(u)\n",
        "        if iu == 0:\n",
        "            continue\n",
        "        # Couleur chaude stable par ID (index choisi via RNG déterministe)\n",
        "        rng2 = np.random.default_rng(iu + 12345)\n",
        "        col = WARM[int(rng2.integers(0, len(WARM)))]\n",
        "        lut[iu] = col\n",
        "    return np.array([lut[int(v)] for v in inst_ids])\n",
        "\n",
        "\n",
        "def _warm_color_by_class() -> dict:\n",
        "    # Associe une couleur chaude à chaque classe THINGS\n",
        "    sids = sorted(list(THINGS_IDS))\n",
        "    return {sid: WARM[i % len(WARM)] for i, sid in enumerate(sids)}\n",
        "\n",
        "\n",
        "def _dark_color_for_class(sid: int) -> str:\n",
        "    # Fallback sur CLASS_COLOR si inconnu, sinon gris foncé\n",
        "    return DARK_STUFF.get(int(sid), CLASS_COLOR.get(int(sid), '#2b2b2b'))\n",
        "\n",
        "\n",
        "def _class_name(sid: int) -> str:\n",
        "    # Nom lisible pour THINGS, sinon fallback explicite\n",
        "    return CLASS_ID_TO_NAME.get(int(sid), f\"stuff({int(sid)})\")\n",
        "\n",
        "\n",
        "def _scatter3d(x, y, z, color, name, opacity, customdata=None, hovertemplate=None):\n",
        "    return go.Scatter3d(\n",
        "        x=x, y=y, z=z,\n",
        "        mode=\"markers\",\n",
        "        marker=dict(size=2, color=color, opacity=opacity),\n",
        "        name=name,\n",
        "        showlegend=False,\n",
        "        customdata=customdata,\n",
        "        hovertemplate=hovertemplate,\n",
        "    )\n",
        "\n",
        "\n",
        "def visualize_triplet(seq_id=\"08\", index=0, decimate=10):\n",
        "    seq_root = Path(DATA_ROOT) / \"sequences\" / seq_id\n",
        "    vel_dir, label_dir, stems = kitti_scan_paths(seq_root)\n",
        "    stem = stems[index]\n",
        "    pts = read_points_bin(vel_dir / f\"{stem}.bin\")\n",
        "\n",
        "    # Oracle (vérité terrain)\n",
        "    gt32 = read_label_file(label_dir / f\"{stem}.label\")\n",
        "    # SemanticKITTI: uint32 pack [inst<<16 | sem]\n",
        "    gt_sem = unpack_semantic(gt32)\n",
        "    # On force l'ID d'instance à partir du 32 bits pour éviter toute ambiguïté\n",
        "    gt_inst = (gt32.astype(np.uint32) >> 16).astype(np.int32)\n",
        "\n",
        "    # Panoptique HGP\n",
        "    pan32 = load_panoptic_pred(seq_id, stem)\n",
        "    pan_sem = unpack_semantic(pan32)\n",
        "    pan_inst = unpack_instance(pan32)\n",
        "\n",
        "    # Décimation pour l'affichage\n",
        "    N = len(pts)\n",
        "    step = max(1, int(decimate))\n",
        "    idx = np.arange(0, N, step)\n",
        "\n",
        "    # Données décimées\n",
        "    pts_d = pts[idx]\n",
        "    gt_sem_d = gt_sem[idx]\n",
        "    gt_inst_d = gt_inst[idx]\n",
        "    pan_sem_d = pan_sem[idx]\n",
        "    pan_inst_d = pan_inst[idx]\n",
        "\n",
        "    # Prépare mapping couleurs\n",
        "    warm_by_class = _warm_color_by_class()\n",
        "\n",
        "    # Figure 2 colonnes (Oracle à gauche, HGP à droite)\n",
        "    fig = make_subplots(rows=1, cols=2, specs=[[{\"type\":\"scene\"}, {\"type\":\"scene\"}]],\n",
        "                        subplot_titles=(\"Oracle\", \"HGP Panoptic\"))\n",
        "\n",
        "    # -------------------\n",
        "    # 1) ORACLE: stuff sombre + things chaudes + hover avec classe et ID oracle\n",
        "    m_stuff = np.isin(gt_sem_d, list(STUFF_IDS))\n",
        "    if np.any(m_stuff):\n",
        "        cols = np.array([_dark_color_for_class(s) for s in gt_sem_d[m_stuff]])\n",
        "        cd = np.stack([\n",
        "            np.vectorize(_class_name)(gt_sem_d[m_stuff]),\n",
        "            gt_sem_d[m_stuff].astype(int),\n",
        "            gt_inst_d[m_stuff].astype(int)\n",
        "        ], axis=1)\n",
        "        ht = \"Oracle · %{customdata[0]}<br>class_id=%{customdata[1]}<br>inst_id=%{customdata[2]}<extra></extra>\"\n",
        "        fig.add_trace(\n",
        "            _scatter3d(pts_d[m_stuff,0], pts_d[m_stuff,1], pts_d[m_stuff,2], cols,\n",
        "                       \"Oracle-stuff\", STUFF_OPACITY, customdata=cd, hovertemplate=ht),\n",
        "            row=1, col=1,\n",
        "        )\n",
        "\n",
        "    m_things = np.isin(gt_sem_d, list(THINGS_IDS))\n",
        "    if np.any(m_things):\n",
        "        cols = np.array([warm_by_class.get(int(s), '#ff9f1c') for s in gt_sem_d[m_things]])\n",
        "        cd = np.stack([\n",
        "            np.vectorize(_class_name)(gt_sem_d[m_things]),\n",
        "            gt_sem_d[m_things].astype(int),\n",
        "            gt_inst_d[m_things].astype(int)\n",
        "        ], axis=1)\n",
        "        ht = \"Oracle · %{customdata[0]}<br>class_id=%{customdata[1]}<br>inst_id=%{customdata[2]}<extra></extra>\"\n",
        "        fig.add_trace(\n",
        "            _scatter3d(pts_d[m_things,0], pts_d[m_things,1], pts_d[m_things,2], cols,\n",
        "                       \"Oracle-things\", THING_OPACITY, customdata=cd, hovertemplate=ht),\n",
        "            row=1, col=1,\n",
        "        )\n",
        "\n",
        "    # -------------------\n",
        "    # 2) HGP PANOPTIC: fond stuff sombre + instances THINGS chaudes + hover avec classe (+ id pour HGP)\n",
        "    # Fond stuff\n",
        "    m_stuff = np.isin(pan_sem_d, list(STUFF_IDS))\n",
        "    if np.any(m_stuff):\n",
        "        cols = np.array([_dark_color_for_class(s) for s in pan_sem_d[m_stuff]])\n",
        "        cd = np.stack([\n",
        "            np.vectorize(_class_name)(pan_sem_d[m_stuff]),\n",
        "            pan_sem_d[m_stuff].astype(int),\n",
        "            pan_inst_d[m_stuff].astype(int)\n",
        "        ], axis=1)\n",
        "        ht = \"HGP · %{customdata[0]}<br>class_id=%{customdata[1]}<br>inst_id=%{customdata[2]}<extra></extra>\"\n",
        "        fig.add_trace(\n",
        "            _scatter3d(pts_d[m_stuff,0], pts_d[m_stuff,1], pts_d[m_stuff,2], cols,\n",
        "                       \"HGP-stuff\", STUFF_OPACITY, customdata=cd, hovertemplate=ht),\n",
        "            row=1, col=2,\n",
        "        )\n",
        "\n",
        "    # Instances THINGS par-dessus (une trace par instance >0)\n",
        "    m_things = np.isin(pan_sem_d, list(THINGS_IDS))\n",
        "    inst_ids = np.unique(pan_inst_d[m_things])\n",
        "    inst_colors_all = make_colors_for_instances(pan_inst_d)\n",
        "    for iid in inst_ids:\n",
        "        if int(iid) <= 0:\n",
        "            continue\n",
        "        m_i = (pan_inst_d == iid) & m_things\n",
        "        if not np.any(m_i):\n",
        "            continue\n",
        "        col_i = inst_colors_all[m_i][0]\n",
        "        cd = np.stack([\n",
        "            np.vectorize(_class_name)(pan_sem_d[m_i]),\n",
        "            pan_sem_d[m_i].astype(int),\n",
        "            np.full(m_i.sum(), int(iid), dtype=int)\n",
        "        ], axis=1)\n",
        "        ht = \"HGP · %{customdata[0]}<br>class_id=%{customdata[1]}<br>inst_id=%{customdata[2]}<extra></extra>\"\n",
        "        fig.add_trace(\n",
        "            _scatter3d(pts_d[m_i,0], pts_d[m_i,1], pts_d[m_i,2], np.full(m_i.sum(), col_i),\n",
        "                       f\"inst {int(iid)}\", THING_OPACITY, customdata=cd, hovertemplate=ht),\n",
        "            row=1, col=2,\n",
        "        )\n",
        "\n",
        "    # Axes/ratio propres: X,Y au sol, Z vers le haut, repère orthonormé\n",
        "    for c in [1, 2]:\n",
        "        fig.update_scenes(\n",
        "            dict(\n",
        "                xaxis=dict(visible=False, title='x'),\n",
        "                yaxis=dict(visible=False, title='y'),\n",
        "                zaxis=dict(visible=False, title='z'),\n",
        "                aspectmode='data',\n",
        "                camera=dict(up=dict(x=0, y=0, z=1)),\n",
        "            ),\n",
        "            row=1, col=c,\n",
        "        )\n",
        "\n",
        "    fig.update_layout(height=600, width=1100, title_text=f\"Sequence {seq_id} · Scan {stem} (1/{len(stems)}, decimate={step})\")\n",
        "    return fig\n",
        "\n",
        "\n",
        "print(\"Visualisation prête. Appelez: visualize_triplet(seq_id, index, decimate)\")\n",
        "# Exemple d'usage:\n",
        "fig = visualize_triplet(\"08\", index=201, decimate=1)\n",
        "fig.show()\n"
      ],
      "id": "DqZXSsPEuG2L"
    }
  ]
}